{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-28T18:25:05.235293Z","iopub.execute_input":"2025-09-28T18:25:05.235651Z","iopub.status.idle":"2025-09-28T18:25:05.242897Z","shell.execute_reply.started":"2025-09-28T18:25:05.235628Z","shell.execute_reply":"2025-09-28T18:25:05.241943Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"%pip install crunch-cli --upgrade --quiet --progress-bar off\n!crunch setup-notebook structural-break TI6EEHIjjshOZbB6k4GqBSaW","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport typing\n\n# Import your dependencies\nimport joblib\nimport pandas as pd\nimport scipy\nimport sklearn.metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T18:25:11.339036Z","iopub.execute_input":"2025-09-28T18:25:11.339329Z","iopub.status.idle":"2025-09-28T18:25:11.346085Z","shell.execute_reply.started":"2025-09-28T18:25:11.339296Z","shell.execute_reply":"2025-09-28T18:25:11.344644Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import crunch\n\n# Load the Crunch Toolings\ncrunch = crunch.load_notebook()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T18:25:11.347596Z","iopub.execute_input":"2025-09-28T18:25:11.347934Z","iopub.status.idle":"2025-09-28T18:25:11.371172Z","shell.execute_reply.started":"2025-09-28T18:25:11.347831Z","shell.execute_reply":"2025-09-28T18:25:11.369938Z"}},"outputs":[{"name":"stdout","text":"loaded inline runner with module: <module '__main__'>\n\ncli version: 8.0.0\navailable ram: 31.35 gb\navailable cpu: 4 core\n----\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Load the data simply\nX_train, y_train, X_test = crunch.load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T18:25:11.373778Z","iopub.execute_input":"2025-09-28T18:25:11.374113Z","iopub.status.idle":"2025-09-28T18:25:15.931026Z","shell.execute_reply.started":"2025-09-28T18:25:11.374088Z","shell.execute_reply":"2025-09-28T18:25:15.930049Z"}},"outputs":[{"name":"stdout","text":"data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_train.parquet: already exists, file length match\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/X_test.reduced.parquet: already exists, file length match\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_train.parquet: already exists, file length match\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\ndata/y_test.reduced.parquet: already exists, file length match\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport typing\nimport joblib\nimport pandas as pd\nimport sklearn.metrics\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import entropy\nimport crunch\n\n# Load the Crunch Toolings\ncrunch = crunch.load_notebook()\n\n# ----------------------------------------------------------------------------------------------------------------------\n# 1. THE TRAIN FUNCTION\n# ----------------------------------------------------------------------------------------------------------------------\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    model_directory_path: str,\n):\n    \"\"\"\n    Trains a LightGBM model with enhanced feature engineering\n    \"\"\"\n\n    def enhanced_preprocess(df: pd.DataFrame):\n        \"\"\"\n        Enhanced preprocessing: group by ID and calculate various statistics and transformations\n        \"\"\"\n        # Basic statistics\n        grouped = df.groupby(level='id').agg({\n            'value': ['mean', 'std', 'min', 'max', 'median', 'skew', pd.Series.kurt,\n                      lambda x: x.quantile(0.05), lambda x: x.quantile(0.95)],\n            'period': ['mean', 'std']\n        })\n        \n        # Flatten the multi-index columns and rename new quantile columns\n        grouped.columns = [f'{col[0]}_{col[1]}' for col in grouped.columns]\n        grouped = grouped.rename(columns={'value_<lambda_0>': 'value_q05', 'value_<lambda_1>': 'value_q95'})\n        \n        # Additional grouped calculations\n        additional_features = df.groupby(level='id').apply(lambda x: pd.Series({\n            'value_range': x['value'].max() - x['value'].min(),\n            'value_iqr': x['value'].quantile(0.75) - x['value'].quantile(0.25),\n            'value_mad': (x['value'] - x['value'].mean()).abs().mean(),\n            'value_abs_mean': np.abs(x['value']).mean(),\n            'value_abs_std': np.abs(x['value']).std(),\n            \n            # Logarithmic transformations (with handling for zero/negative values)\n            'value_log_mean': np.log(np.abs(x['value']) + 1e-10).mean(),\n            'value_log_std': np.log(np.abs(x['value']) + 1e-10).std(),\n            \n            # Exponential transformations\n            'value_exp_mean': np.exp(x['value']).mean(),\n            'value_exp_std': np.exp(x['value']).std(),\n            \n            # Square transformations\n            'value_sq_mean': (x['value'] ** 2).mean(),\n            'value_sq_std': (x['value'] ** 2).std(),\n            \n            # Square root transformations (with handling for negative values)\n            'value_sqrt_mean': np.sqrt(np.abs(x['value'])).mean(),\n            'value_sqrt_std': np.sqrt(np.abs(x['value'])).std(),\n            \n            # Multiplication features\n            'value_period_product_mean': (x['value'] * x['period']).mean(),\n            'value_period_product_std': (x['value'] * x['period']).std(),\n            \n            # Division features (with handling for division by zero)\n            'value_period_ratio_mean': (x['value'] / (x['period'] + 1e-10)).mean(),\n            'value_period_ratio_std': (x['value'] / (x['period'] + 1e-10)).std(),\n            \n            # Interaction features\n            'value_mean_times_period_mean': x['value'].mean() * x['period'].mean(),\n            'value_std_times_period_std': x['value'].std() * x['period'].std(),\n            \n            # Count features\n            'total_observations': len(x),\n            'period_0_count': (x['period'] == 0).sum(),\n            'period_1_count': (x['period'] == 1).sum(),\n            'period_ratio': (x['period'] == 1).sum() / len(x) if len(x) > 0 else 0,\n            \n            # Time-based features\n            'time_correlation': x.reset_index().corr()['time']['value'] if len(x) > 1 else 0,\n            'value_time_slope': np.polyfit(x.reset_index()['time'], x['value'], 1)[0] if len(x) > 1 else 0,\n            \n            # ADVANCED MATH FEATURES\n            'value_entropy': entropy(x['value'].value_counts(normalize=True)) if len(x['value'].unique()) > 1 else 0,\n            'period_entropy': entropy(x['period'].value_counts(normalize=True)) if len(x['period'].unique()) > 1 else 0,\n            \n            # Fourier Transform Features: Capture periodicity\n            'fft_max_freq_amp': np.abs(np.fft.fft(x['value'].values)[1:len(x['value'])//2]).max() if len(x) > 1 else 0,\n            'fft_mean_freq_amp': np.abs(np.fft.fft(x['value'].values)[1:len(x['value'])//2]).mean() if len(x) > 1 else 0,\n            \n            # Difference features: Capture change rates\n            'value_diff_mean': x['value'].diff().mean() if len(x) > 1 else 0,\n            'value_diff_std': x['value'].diff().std() if len(x) > 1 else 0,\n            \n            # Lagged features\n            'value_lag_1_corr': x['value'].corr(x['value'].shift(1)) if len(x) > 2 else 0,\n\n            # NEW FEATURES\n            # Coefficient of Variation (with handling for mean == 0)\n            'value_cv': x['value'].std() / (x['value'].mean() + 1e-10),\n            \n            # Signal-to-Noise Ratio (with handling for std == 0)\n            'value_snr': x['value'].mean() / (x['value'].std() + 1e-10),\n\n            # Time-based transformations\n            'period_sin': np.sin(2 * np.pi * x['period']).mean(),\n            'period_cos': np.cos(2 * np.pi * x['period']).mean(),\n            \n            # Rolling statistics (using a 3-period window)\n            'value_rolling_mean_3': x['value'].rolling(window=3, min_periods=1).mean().iloc[-1] if len(x) >= 1 else 0,\n            'value_rolling_std_3': x['value'].rolling(window=3, min_periods=1).std().iloc[-1] if len(x) >= 1 else 0,\n            \n        }))\n        \n        # Combine all features\n        result = pd.concat([grouped, additional_features], axis=1)\n        \n        # Replace infinite values with large finite numbers\n        result = result.replace([np.inf, -np.inf], np.nan)\n        result = result.fillna(0)\n        \n        return result\n\n    # Print class distribution before data preprocessing\n    print(\"Class Distribution Before Preprocessing:\")\n    print(y_train.value_counts())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    # Process training data - enhanced preprocessing\n    X_train_processed = enhanced_preprocess(X_train)\n    \n    # Ensure y_train is aligned with processed X_train\n    y_train_aligned = y_train.loc[X_train_processed.index].astype(int)\n\n    # Print class distribution after data preprocessing\n    print(\"Class Distribution After Preprocessing:\")\n    print(y_train_aligned.value_counts())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    # Print shape information\n    print(f\"X_train shape: {X_train.shape}\")\n    print(f\"X_train_processed shape: {X_train_processed.shape}\")\n    print(f\"y_train_aligned shape: {y_train_aligned.shape}\")\n    print(f\"\\nNumber of features: {len(X_train_processed.columns)}\")\n    print(\"Features:\", X_train_processed.columns.tolist())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    # Split into train and validation sets\n    X_train_split, X_val, y_train_split, y_val = train_test_split(\n        X_train_processed, y_train_aligned, test_size=0.2, random_state=42, stratify=y_train_aligned\n    )\n\n    # Scale features\n    scaler = StandardScaler()\n    X_train_split_scaled = scaler.fit_transform(X_train_split)\n    X_val_scaled = scaler.transform(X_val)\n\n    # Create LightGBM datasets\n    train_data = lgb.Dataset(X_train_split_scaled, label=y_train_split)\n    val_data = lgb.Dataset(X_val_scaled, label=y_val, reference=train_data)\n\n    # Define LightGBM parameters\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'num_leaves': 160,\n        'learning_rate': 0.09,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'verbose': -1,\n        'random_state': 42,\n        'max_depth': 20,\n        'lambda_l1': 5,  # L1 regularization\n        'lambda_l2': 4,  # L2 regularization\n\n    }\n\n    # Train the model with early stopping\n    model = lgb.train(\n        params,\n        train_data,\n        num_boost_round=1500,\n        valid_sets=[val_data],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50, verbose=True),\n        ]\n    )\n\n    # Save model and scaler\n    os.makedirs(model_directory_path, exist_ok=True)\n    model_path = os.path.join(model_directory_path, 'lgb_model.txt')\n    model.save_model(model_path)\n    \n    # Save scaler\n    scaler_path = os.path.join(model_directory_path, 'scaler.pkl')\n    joblib.dump(scaler, scaler_path)\n    \n    # Save feature names for inference\n    feature_names_path = os.path.join(model_directory_path, 'feature_names.pkl')\n    joblib.dump(X_train_processed.columns.tolist(), feature_names_path)\n\n    # Training metrics\n    X_train_scaled = scaler.transform(X_train_processed)\n    y_train_pred = model.predict(X_train_scaled)\n    roc_auc = sklearn.metrics.roc_auc_score(y_train_aligned, y_train_pred)\n    print(f\"Training ROC AUC: {roc_auc:.4f}\")\n    print(\"Classification Report (Train):\")\n    print(sklearn.metrics.classification_report(y_train_aligned, (y_train_pred > 0.5).astype(int)))\n    \n    return model, scaler\n\n# ----------------------------------------------------------------------------------------------------------------------\n# 2. THE INFER FUNCTION\n# ----------------------------------------------------------------------------------------------------------------------\n\ndef infer(\n    X_test: typing.Iterable[pd.DataFrame],\n    model_directory_path: str,\n):\n    \"\"\"\n    Makes predictions using the trained LightGBM model with enhanced preprocessing\n    \"\"\"\n    \n    def enhanced_preprocess_infer(df: pd.DataFrame):\n        \"\"\"\n        Enhanced preprocessing for inference\n        \"\"\"\n        # Basic statistics\n        grouped = df.groupby(level='id').agg({\n            'value': ['mean', 'std', 'min', 'max', 'median', 'skew', pd.Series.kurt,\n                      lambda x: x.quantile(0.05), lambda x: x.quantile(0.95)],\n            'period': ['mean', 'std']\n        })\n        \n        # Flatten the multi-index columns and rename new quantile columns\n        grouped.columns = [f'{col[0]}_{col[1]}' for col in grouped.columns]\n        grouped = grouped.rename(columns={'value_<lambda_0>': 'value_q05', 'value_<lambda_1>': 'value_q95'})\n        \n        # Additional grouped calculations\n        additional_features = df.groupby(level='id').apply(lambda x: pd.Series({\n            'value_range': x['value'].max() - x['value'].min(),\n            'value_iqr': x['value'].quantile(0.75) - x['value'].quantile(0.25),\n            'value_mad': (x['value'] - x['value'].mean()).abs().mean(),\n            'value_abs_mean': np.abs(x['value']).mean(),\n            'value_abs_std': np.abs(x['value']).std(),\n            'value_log_mean': np.log(np.abs(x['value']) + 1e-10).mean(),\n            'value_log_std': np.log(np.abs(x['value']) + 1e-10).std(),\n            'value_exp_mean': np.exp(x['value']).mean(),\n            'value_exp_std': np.exp(x['value']).std(),\n            'value_sq_mean': (x['value'] ** 2).mean(),\n            'value_sq_std': (x['value'] ** 2).std(),\n            'value_sqrt_mean': np.sqrt(np.abs(x['value'])).mean(),\n            'value_sqrt_std': np.sqrt(np.abs(x['value'])).std(),\n            'value_period_product_mean': (x['value'] * x['period']).mean(),\n            'value_period_product_std': (x['value'] * x['period']).std(),\n            'value_period_ratio_mean': (x['value'] / (x['period'] + 1e-10)).mean(),\n            'value_period_ratio_std': (x['value'] / (x['period'] + 1e-10)).std(),\n            'value_mean_times_period_mean': x['value'].mean() * x['period'].mean(),\n            'value_std_times_period_std': x['value'].std() * x['period'].std(),\n            'total_observations': len(x),\n            'period_0_count': (x['period'] == 0).sum(),\n            'period_1_count': (x['period'] == 1).sum(),\n            'period_ratio': (x['period'] == 1).sum() / len(x) if len(x) > 0 else 0,\n            'time_correlation': x.reset_index().corr()['time']['value'] if len(x) > 1 else 0,\n            'value_time_slope': np.polyfit(x.reset_index()['time'], x['value'], 1)[0] if len(x) > 1 else 0,\n            \n            # ADVANCED MATH FEATURES\n            'value_entropy': entropy(x['value'].value_counts(normalize=True)) if len(x['value'].unique()) > 1 else 0,\n            'period_entropy': entropy(x['period'].value_counts(normalize=True)) if len(x['period'].unique()) > 1 else 0,\n            'fft_max_freq_amp': np.abs(np.fft.fft(x['value'].values)[1:len(x['value'])//2]).max() if len(x) > 1 else 0,\n            'fft_mean_freq_amp': np.abs(np.fft.fft(x['value'].values)[1:len(x['value'])//2]).mean() if len(x) > 1 else 0,\n            'value_diff_mean': x['value'].diff().mean() if len(x) > 1 else 0,\n            'value_diff_std': x['value'].diff().std() if len(x) > 1 else 0,\n            'value_lag_1_corr': x['value'].corr(x['value'].shift(1)) if len(x) > 2 else 0,\n\n            # NEW FEATURES\n            'value_cv': x['value'].std() / (x['value'].mean() + 1e-10),\n            'value_snr': x['value'].mean() / (x['value'].std() + 1e-10),\n            'period_sin': np.sin(2 * np.pi * x['period']).mean(),\n            'period_cos': np.cos(2 * np.pi * x['period']).mean(),\n            'value_rolling_mean_3': x['value'].rolling(window=3, min_periods=1).mean().iloc[-1] if len(x) >= 1 else 0,\n            'value_rolling_std_3': x['value'].rolling(window=3, min_periods=1).std().iloc[-1] if len(x) >= 1 else 0,\n            \n        }))\n        \n        # Combine all features\n        result = pd.concat([grouped, additional_features], axis=1)\n        \n        # Replace infinite values\n        result = result.replace([np.inf, -np.inf], np.nan)\n        result = result.fillna(0)\n        \n        return result\n\n    # Load the trained model\n    model_path = os.path.join(model_directory_path, 'lgb_model.txt')\n    model = lgb.Booster(model_file=model_path)\n    \n    # Load scaler\n    scaler_path = os.path.join(model_directory_path, 'scaler.pkl')\n    scaler = joblib.load(scaler_path)\n    \n    # Load feature names\n    feature_names_path = os.path.join(model_directory_path, 'feature_names.pkl')\n    feature_names = joblib.load(feature_names_path)\n\n    # Yield once before the loop (required by Crunch framework)\n    yield\n\n    for dataset in X_test:\n        # Preprocess the test data\n        X_test_processed = enhanced_preprocess_infer(dataset)\n        \n        # Ensure the test data has the same columns as training data\n        X_test_processed = X_test_processed.reindex(columns=feature_names, fill_value=0)\n        \n        # Scale features\n        X_test_scaled = scaler.transform(X_test_processed)\n        \n        # Make prediction\n        prediction = model.predict(X_test_scaled)[0]\n        \n        yield prediction\n\n# ----------------------------------------------------------------------------------------------------------------------\n# 3. LOCAL TESTING\n# ----------------------------------------------------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    # Load data\n    X_train, y_train, X_test = crunch.load_data()\n    \n    # Train the model\n    model_directory_path = \"models\"\n    model, scaler = train(X_train, y_train, model_directory_path)\n    \n    # Test using crunch\n    crunch.test()\n    \n    # Load saved predictions and evaluate\n    try:\n        prediction = pd.read_parquet(\"data/prediction.parquet\")\n        target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n\n        roc_auc = sklearn.metrics.roc_auc_score(target, prediction)\n        print(f\"Local Test ROC AUC Score: {roc_auc:.4f}\")\n        print(\"Classification Report (Test):\")\n        print(sklearn.metrics.classification_report(target, (prediction > 0.5).astype(int)))\n    except Exception as e:\n        print(f\"Error during evaluation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T18:57:18.828852Z","iopub.execute_input":"2025-09-28T18:57:18.829237Z","iopub.status.idle":"2025-09-28T19:02:18.234326Z","shell.execute_reply.started":"2025-09-28T18:57:18.829211Z","shell.execute_reply":"2025-09-28T19:02:18.233239Z"}},"outputs":[{"name":"stdout","text":"loaded inline runner with module: <module '__main__'>\n\ncli version: 8.0.0\navailable ram: 31.35 gb\navailable cpu: 4 core\n----\ndata/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_train.parquet: already exists, file length match\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/X_test.reduced.parquet: already exists, file length match\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_train.parquet: already exists, file length match\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\ndata/y_test.reduced.parquet: already exists, file length match\nClass Distribution Before Preprocessing:\nstructural_breakpoint\nFalse    7092\nTrue     2909\nName: count, dtype: int64\n\n==================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py:1016: RuntimeWarning: overflow encountered in square\n  sqr = _ensure_numeric((avg - values) ** 2)\n","output_type":"stream"},{"name":"stdout","text":"Class Distribution After Preprocessing:\nstructural_breakpoint\n0    7092\n1    2909\nName: count, dtype: int64\n\n==================================================\n\nX_train shape: (23715734, 2)\nX_train_processed shape: (10001, 49)\ny_train_aligned shape: (10001,)\n\nNumber of features: 49\nFeatures: ['value_mean', 'value_std', 'value_min', 'value_max', 'value_median', 'value_skew', 'value_kurt', 'value_q05', 'value_q95', 'period_mean', 'period_std', 'value_range', 'value_iqr', 'value_mad', 'value_abs_mean', 'value_abs_std', 'value_log_mean', 'value_log_std', 'value_exp_mean', 'value_exp_std', 'value_sq_mean', 'value_sq_std', 'value_sqrt_mean', 'value_sqrt_std', 'value_period_product_mean', 'value_period_product_std', 'value_period_ratio_mean', 'value_period_ratio_std', 'value_mean_times_period_mean', 'value_std_times_period_std', 'total_observations', 'period_0_count', 'period_1_count', 'period_ratio', 'time_correlation', 'value_time_slope', 'value_entropy', 'period_entropy', 'fft_max_freq_amp', 'fft_mean_freq_amp', 'value_diff_mean', 'value_diff_std', 'value_lag_1_corr', 'value_cv', 'value_snr', 'period_sin', 'period_cos', 'value_rolling_mean_3', 'value_rolling_std_3']\n\n==================================================\n\nTraining until validation scores don't improve for 50 rounds\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1066: RuntimeWarning: overflow encountered in square\n  temp **= 2\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: overflow encountered in square\n  new_unnormalized_variance -= correction**2 / new_sample_count\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in subtract\n  new_unnormalized_variance -= correction**2 / new_sample_count\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py:86: RuntimeWarning: overflow encountered in square\n  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py:87: RuntimeWarning: invalid value encountered in less_equal\n  return var <= upper_bound\n","output_type":"stream"},{"name":"stdout","text":"Early stopping, best iteration is:\n[67]\tvalid_0's auc: 0.665891\nTraining ROC AUC: 0.9224\nClassification Report (Train):\n              precision    recall  f1-score   support\n\n           0       0.81      0.99      0.89      7092\n           1       0.94      0.42      0.58      2909\n\n    accuracy                           0.82     10001\n   macro avg       0.87      0.70      0.73     10001\nweighted avg       0.84      0.82      0.80     10001\n\n","output_type":"stream"},{"name":"stderr","text":"18:59:43 \n18:59:44 started\n18:59:44 running local test\n18:59:44 internet access isn't restricted, no check will be done\n18:59:44 \n18:59:45 starting unstructured loop...\n18:59:45 executing - command=train\n","output_type":"stream"},{"name":"stdout","text":"data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_train.parquet: already exists, file length match\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/X_test.reduced.parquet: already exists, file length match\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_train.parquet: already exists, file length match\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\ndata/y_test.reduced.parquet: already exists, file length match\nClass Distribution Before Preprocessing:\nstructural_breakpoint\nFalse    7092\nTrue     2909\nName: count, dtype: int64\n\n==================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py:1016: RuntimeWarning: overflow encountered in square\n  sqr = _ensure_numeric((avg - values) ** 2)\n","output_type":"stream"},{"name":"stdout","text":"Class Distribution After Preprocessing:\nstructural_breakpoint\n0    7092\n1    2909\nName: count, dtype: int64\n\n==================================================\n\nX_train shape: (23715734, 2)\nX_train_processed shape: (10001, 49)\ny_train_aligned shape: (10001,)\n\nNumber of features: 49\nFeatures: ['value_mean', 'value_std', 'value_min', 'value_max', 'value_median', 'value_skew', 'value_kurt', 'value_q05', 'value_q95', 'period_mean', 'period_std', 'value_range', 'value_iqr', 'value_mad', 'value_abs_mean', 'value_abs_std', 'value_log_mean', 'value_log_std', 'value_exp_mean', 'value_exp_std', 'value_sq_mean', 'value_sq_std', 'value_sqrt_mean', 'value_sqrt_std', 'value_period_product_mean', 'value_period_product_std', 'value_period_ratio_mean', 'value_period_ratio_std', 'value_mean_times_period_mean', 'value_std_times_period_std', 'total_observations', 'period_0_count', 'period_1_count', 'period_ratio', 'time_correlation', 'value_time_slope', 'value_entropy', 'period_entropy', 'fft_max_freq_amp', 'fft_mean_freq_amp', 'value_diff_mean', 'value_diff_std', 'value_lag_1_corr', 'value_cv', 'value_snr', 'period_sin', 'period_cos', 'value_rolling_mean_3', 'value_rolling_std_3']\n\n==================================================\n\nTraining until validation scores don't improve for 50 rounds\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1066: RuntimeWarning: overflow encountered in square\n  temp **= 2\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: overflow encountered in square\n  new_unnormalized_variance -= correction**2 / new_sample_count\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in subtract\n  new_unnormalized_variance -= correction**2 / new_sample_count\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py:86: RuntimeWarning: overflow encountered in square\n  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py:87: RuntimeWarning: invalid value encountered in less_equal\n  return var <= upper_bound\n19:02:14 executing - command=infer\n","output_type":"stream"},{"name":"stdout","text":"Early stopping, best iteration is:\n[67]\tvalid_0's auc: 0.665891\nTraining ROC AUC: 0.9224\nClassification Report (Train):\n              precision    recall  f1-score   support\n\n           0       0.81      0.99      0.89      7092\n           1       0.94      0.42      0.58      2909\n\n    accuracy                           0.82     10001\n   macro avg       0.87      0.70      0.73     10001\nweighted avg       0.84      0.82      0.80     10001\n\n","output_type":"stream"},{"name":"stderr","text":"19:02:17 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n19:02:17 executing - command=infer\n19:02:18 determinism check: passed\n19:02:18 save prediction - path=data/prediction.parquet\n19:02:18 ended\n19:02:18 duration - time=00:02:34\n19:02:18 memory - before=\"1.07 GB\" after=\"1.07 GB\" consumed=\"237.57 KB\"\n","output_type":"stream"},{"name":"stdout","text":"Local Test ROC AUC Score: 0.6653\nClassification Report (Test):\n              precision    recall  f1-score   support\n\n       False       0.73      0.96      0.83        71\n        True       0.62      0.17      0.26        30\n\n    accuracy                           0.72       101\n   macro avg       0.68      0.56      0.55       101\nweighted avg       0.70      0.72      0.66       101\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}